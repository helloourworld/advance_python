{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jieba分词《二》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. 词频统计、降序排序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache c:\\users\\david\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 0.435 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要前多少位高频词？ 3\n",
      "， 24\n",
      "的 20\n",
      "  11\n"
     ]
    }
   ],
   "source": [
    "article = open(\"C_jieba_demo.txt\", \"r\").read()\n",
    "words = jieba.cut(article, cut_all = False)\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "freq_word = []\n",
    "for word, freq in word_freq.items():\n",
    "    freq_word.append((word, freq))\n",
    "freq_word.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "max_number = int(raw_input(u\"需要前多少位高频词？ \"))\n",
    "\n",
    "for word, freq in freq_word[: max_number]:\n",
    "    print word, freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%^&**( argly停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. 人工去停用词\n",
    "\n",
    "标点符号、虚词、连词不在统计范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "for word in open(\"D:\\\\python_workspace\\\\toolkits\\\\stopword1.txt\",\"r\"):\n",
    "    stopwords.append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = open(\"C_jieba_demo.txt\", \"r\").read()\n",
    "words = jieba.cut(article, cut_all = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ 退市 制度 推出 新 三板 完善 制度 建设 重要环节 ，   退市 制度 市场化 常态 化是 大势所趋 。 市场化 ， 企业 自主 决定 ， 主动 退出 。 包括 并购 整合 ，   IPO   摘牌 ， 企业 认为 适合 继续 挂牌 原因 ， 选择 主动 摘牌 。 常态 化 将 没有 持续 经营 能力 违法 违规 企业 清 退出 市场 。   去年 ， 新 三板 迎来 扩容 ， 挂牌 门槛 偏低 ，   部分 企业 已经 挂牌 ， 规范 意识 较差 ， 不能 适应 后续 监管 规则 ， 这种 严格 化 信息 披露 要求 高压 式 监管 成为 企业 负担 ， 部分 企业 新 三板 “ 站 桩 ” 耗费 资源 主动 退出 ， 预计 未来 新 三板 企业 退市 将 实现 市场化 ， 这是 解决 挂牌 企业 质量 参差不齐 、 保证 新 三板 健康 发展 有效 手段 。   退市 常态 化 需要 政策 层面 配合 ， 四季度 股转 系统 制定 出台 挂牌 公司 终止 挂牌 实施细则 ， 建立 常态 化 退出 机制 ， 实现 市场 优胜劣汰 ， 提高 挂牌 公司 整体 质量 。     2016   年 中期 投资 策略 报告 中 ， 退市 制度 落地 抱 较 高 预期 。 事实上 ， 退市 制度 成为   2016   下半年 出台 具有 影响力 新政 。 监管部门 选择 上市公司 接近 一万家 规模 时候 出台 退市 制度 ， 合理 控制 挂牌 总体 数量 意义 里面 ， 时机 恰到好处 ， 较为 明显 效果 。 \n"
     ]
    }
   ],
   "source": [
    "stayed_line = \"\"\n",
    "for word in words:\n",
    "    if word.encode(\"utf-8\") not in stopwords:\n",
    "        stayed_line += word + \" \"\n",
    "print stayed_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. 合并同义词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 将同义词列举出来，按下Tab键分隔，把第一个词作为需要显示的词语，后面的词语作为要替代的同义词，一系列同义词放在一行。\n",
    "* 这里，“北京”、“首都”、“京城”、“北平城”、“故都”为同义词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_dict = {}\n",
    "\n",
    "for line in open(\"C_jieba_tongyici.txt\", \"r\"):\n",
    "    seperate_word = line.strip().split(\"\\t\")\n",
    "    num = len(seperate_word)\n",
    "    for i in range(1, num):\n",
    "        combine_dict[seperate_word[i]] = seperate_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\xe4\\xba\\xac\\xe5\\x9f\\x8e': '\\xef\\xbb\\xbf\\xe5\\x8c\\x97\\xe4\\xba\\xac',\n",
       " '\\xe5\\x8c\\x97\\xe5\\xb9\\xb3\\xe5\\x9f\\x8e': '\\xef\\xbb\\xbf\\xe5\\x8c\\x97\\xe4\\xba\\xac',\n",
       " '\\xe6\\x95\\x85\\xe9\\x83\\xbd': '\\xef\\xbb\\xbf\\xe5\\x8c\\x97\\xe4\\xba\\xac',\n",
       " '\\xe9\\x9c\\xbe\\xe9\\x83\\xbd': '\\xef\\xbb\\xbf\\xe5\\x8c\\x97\\xe4\\xba\\xac',\n",
       " '\\xe9\\xa6\\x96\\xe9\\x83\\xbd': '\\xef\\xbb\\xbf\\xe5\\x8c\\x97\\xe4\\xba\\xac'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京是中国的﻿北京，﻿北京的景色非常优美，就像当年的﻿北京，我爱这﻿北京的一草一木。\n"
     ]
    }
   ],
   "source": [
    "jieba.suggest_freq(\"北平城\", tune = True)\n",
    "seg_list = jieba.cut(\"北京是中国的首都，京城的景色非常优美，就像当年的北平城，我爱这故都的一草一木。\", cut_all = False)\n",
    "f = \",\".join(seg_list)\n",
    "result = open(\"output.txt\", \"w\")\n",
    "result.write(f.encode(\"utf-8\"))\n",
    "result.close()\n",
    "\n",
    "for line in open(\"output.txt\", \"r\"):\n",
    "    line_1 = line.split(\",\")\n",
    "\n",
    "final_sentence = \"\"\n",
    "for word in line_1:\n",
    "    if word in combine_dict:\n",
    "        word = combine_dict[word]\n",
    "        final_sentence += word\n",
    "    else:\n",
    "        final_sentence += word\n",
    "print final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. 词语提及率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要步骤：分词——过滤停用词（略）——替代同义词——计算词语在文本中出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. 按词性提取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好帅 n\n",
      "， x\n",
      "能力 n\n",
      "超强 v\n",
      "， x\n",
      "是 v\n",
      "“ x\n",
      "牛 n\n",
      "” x\n",
      "， x\n",
      "是 v\n",
      "能力 n\n",
      "， x\n",
      "队里 n\n",
      "贴心 v\n",
      "晨 n\n",
      "妈妈 n\n",
      "。 x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "word = pseg.cut(\"李晨好帅，又能力超强，是“大黑牛”，也是一个能力者，还是队里贴心的晨妈妈。\")\n",
    "for w in word:\n",
    "    if w.flag in [\"n\", \"v\", \"x\"]:\n",
    "        print w.word, w.flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
